{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7155a8ae-767b-4b78-af6e-123075a3ab3d",
   "metadata": {},
   "source": [
    "#Clustering Using K-Means and DBSCANS Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004a6993-3499-4262-a1bf-fb0a86ba7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libaries\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import socket\n",
    "import struct\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2069b7-823b-4df3-bcbd-07ea7928ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"\"\n",
    "\n",
    "def get_prevectors(data_path):\n",
    "    prevectors = {}\n",
    "    for path in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, path)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    ip, request_type, response_code = LOG_REGEX.findall(line)[0]\n",
    "                    ip = ip2int(ip)\n",
    "                    if ip not in prevectors:\n",
    "                        prevectors[ip] = {\"requests\": {}, \"responses\": {}}\n",
    "                    if request_type not in prevectors[ip][\"requests\"]:\n",
    "                        prevectors[ip][\"requests\"][request_type] = 0\n",
    "                    prevectors[ip][\"requests\"][request_type] += 1\n",
    "                    if response_code not in prevectors[ip][\"responses\"]:\n",
    "                        prevectors[ip][\"responses\"][response_code] = 0\n",
    "                    prevectors[ip][\"responses\"][response_code] += 1\n",
    "                except IndexError:\n",
    "                    continue\n",
    "    return prevectors\n",
    "\n",
    "\n",
    "def convert_prevectors_to_vectors(prevectors):\n",
    "    request_types = [\n",
    "        \"GET\",\n",
    "        \"POST\",\n",
    "        \"HEAD\",\n",
    "        \"OPTIONS\",\n",
    "        \"PUT\",\n",
    "        \"TRACE\"\n",
    "    ]\n",
    "    response_codes = [\n",
    "        200,\n",
    "        404,\n",
    "        403,\n",
    "        304,\n",
    "        301,\n",
    "        206,\n",
    "        418,\n",
    "        416,\n",
    "        403,\n",
    "        405,\n",
    "        503,\n",
    "        500,\n",
    "    ]\n",
    "\n",
    "    vectors = np.zeros((len(prevectors.keys()), len(request_types) + len(response_codes)), dtype=np.float32)\n",
    "    ips = []\n",
    "\n",
    "    for index, (k, v) in enumerate(prevectors.items()):\n",
    "        ips.append(k)\n",
    "        for ri, r in enumerate(request_types):\n",
    "            if r in v[\"requests\"]:\n",
    "                vectors[index, ri] = v[\"requests\"][r]\n",
    "        for ri, r in enumerate(response_codes):\n",
    "            if r in v[\"responses\"]:\n",
    "                vectors[index, len(request_types) + ri] = v[\"requests\"][r]\n",
    "\n",
    "    return ips, vectors\n",
    "\n",
    "\n",
    "def create_secrepo(data_path):\n",
    "    prevectors = get_prevectors(data_path)\n",
    "    ips, vectors = convert_prevectors_to_vectors(prevectors)\n",
    "    scaler = MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d5a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Graphing our Vectors\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import h5py\n",
    "\n",
    "\n",
    "def visualize(vectors):\n",
    "    pca = PCA(n_components=3)\n",
    "    projected_vectors = pca.fit_transform(vectors)\n",
    "    print projected_vectors.shape\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plt.scatter(\n",
    "        projected_vectors[:, 0],\n",
    "        projected_vectors[:, 1],\n",
    "        zs=projected_vectors[:, 2],\n",
    "        s=200,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "visualize(projected_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: First pass clustering with K-means\n",
    "import h5py\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-c\", \"--cluster_method\", choices=[\"kmeans\", \"dbscan\"], default=\"kmeans\")\n",
    "    parser.add_argument('-n', \"--number_clusters\", type=int, default=2)\n",
    "    parser.add_argument('-e', '--epsilon', type=float, default=6)\n",
    "    parser.add_argument('-m', '--number_points', type=int, default=1)\n",
    "    parser.add_argument(\"-i\", \"--vectors\", required=True, help=\"HDF5 file containing the vectors\")\n",
    "    parser.add_argument(\"-o\", \"--output\", required=True, help=\"Output HDF5 containing the vectors\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cluster_method = args.cluster_method\n",
    "    path = args.vectors\n",
    "    output_path = args.output\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        vectors = f[\"vectors\"][:]\n",
    "        ips = f[\"notes\"][:]\n",
    "\n",
    "    if cluster_method == \"kmeans\":\n",
    "        number_clusters = args.number_clusters\n",
    "        kmeans = KMeans(n_clusters=number_clusters)\n",
    "        clusters = kmeans.fit_predict(vectors)\n",
    "    elif cluster_method == \"dbscan\":\n",
    "        epsilon = args.epsilon\n",
    "        number_points = args.number_points\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=number_points)\n",
    "        clusters = dbscan.fit_predict(vectors)\n",
    "\n",
    "    counter = Counter(clusters.tolist())\n",
    "    for key in sorted(counter.keys()):\n",
    "        print \"Label {0} has {1} samples\".format(key, counter[key])\n",
    "\n",
    "    # create new hdf5 with clusters added\n",
    "    with h5py.File(output_path, \"w\") as f:\n",
    "        f.create_dataset(\"vectors\", shape=vectors.shape, data=vectors)\n",
    "        f.create_dataset(\"cluster\", shape=(vectors.shape[0],), data=clusters, dtype=np.int32)\n",
    "        f.create_dataset(\"notes\", shape=(vectors.shape[0],), data=np.array(ips))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca3fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Validating our Clusters Statistically\n",
    "import h5py\n",
    "import socket\n",
    "import struct\n",
    "from sklearn.metrics import pairwise_distances, silhouette_samples, silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def int2ip(addr):\n",
    "    return socket.inet_ntoa(struct.pack(\"!I\", addr))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", \"--vectors\", required=True, help=\"HDF5 file containing the vectors\")\n",
    "    parser.add_argument(\"-l\", \"--label\", required=False, default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    path = args.vectors\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        vectors = f[\"vectors\"][:]\n",
    "        ips = f[\"notes\"][:]\n",
    "        clusters = f[\"cluster\"][:]\n",
    "\n",
    "    ips = map(int2ip, ips.tolist())\n",
    "\n",
    "    print \"Vectors shape:\", vectors.shape\n",
    "    print \"Minimum feature value:\", vectors.min()\n",
    "    print \"Mean feature value:\", vectors.mean()\n",
    "    print \"Max feature value:\", vectors.max()\n",
    "    print \"Percentage of null values:\", 100.0 * (float((vectors == 0).sum()) / (vectors.shape[0] * vectors.shape[1]))\n",
    "    print \"\"\n",
    "\n",
    "    vector_distances = pairwise_distances(vectors)\n",
    "    print \"Minimum distance between vectors:\", vector_distances.min()\n",
    "    print \"Mean distance between vectors:\", vector_distances.mean()\n",
    "    print \"Maximum distance between vectors:\", vector_distances.max()\n",
    "    print \"\"\n",
    "\n",
    "    silhouette_scores = silhouette_samples(vectors, clusters)\n",
    "    centroid_distances = []\n",
    "\n",
    "    print \"Number of labels:\", len(set(clusters.tolist()))\n",
    "    for label in sorted(set(clusters.tolist())):\n",
    "        n_vects = vectors[clusters == label, :]\n",
    "        centroid = n_vects.mean(0)\n",
    "        centroid_distances.extend(pairwise_distances(centroid.reshape(1, -1), n_vects).tolist()[0])\n",
    "        distances = pairwise_distances(n_vects)\n",
    "        scores = silhouette_scores[clusters == label]\n",
    "\n",
    "        print \"Number of items in label {0}: {1}  ({2}%) (avg dist: {3}) (avg silhouette: {4})\".format(\n",
    "            label,\n",
    "            n_vects.shape[0],\n",
    "            (100.0 * n_vects.shape[0]) / vectors.shape[0],\n",
    "            distances.mean(),\n",
    "            scores.mean()\n",
    "        )\n",
    "    print \"\"\n",
    "\n",
    "    centroid_distances = np.array(centroid_distances)\n",
    "    print \"Minimum label centroid distance:\", centroid_distances.min()\n",
    "    print \"Mean label centroid distance:\", centroid_distances.mean()\n",
    "    print \"Max label centroid distance:\", centroid_distances.max()\n",
    "    print \"Overall Silhouette Score\", silhouette_score(vector_distances, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2aa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Inspecting our Clusters\n",
    "import h5py\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "\n",
    "def int2ip(addr):\n",
    "    return socket.inet_ntoa(struct.pack(\"!I\", addr))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", \"--vectors\", required=True, help=\"HDF5 file containing the vectors\")\n",
    "    parser.add_argument(\"-l\", \"--label\", type=int, required=False, default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    path = args.vectors\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        vectors = f[\"vectors\"][:]\n",
    "        ips = f[\"notes\"][:]\n",
    "        clusters = f[\"cluster\"][:]\n",
    "\n",
    "    if args.label is None:\n",
    "        for cluster_id in sorted(set(clusters.tolist())):\n",
    "            for ip in ips[clusters == cluster_id]:\n",
    "                print cluster_id, int2ip(ip)\n",
    "    else:\n",
    "        cluster_id = args.label\n",
    "        for ip in ips[clusters == cluster_id]:\n",
    "            print cluster_id, int2ip(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Modifying K to optimize Cluster Results\n",
    "python cluster_vectors.py -c kmeans -n 12 -i secrepo.h5 -o secrepo.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Repeating our INspection and Vlaidation Procedures\n",
    "python label_notes.py -i secrepo.h5|grep70.32.104.50\n",
    "\n",
    "#Step 8: Validate the cluster using Silhouette Scoring\n",
    "python stats_vectors.py secrepo.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9: Our next step is to see what IP addresses have been doing by tracking their activity in web server logs \n",
    "python label_notes.py -i secrepo.h5 -l <label>\n",
    "\n",
    "#use grep to search through logs and siplay entires inwhich IP addresses appear \n",
    "grep -ar 70;.32.104.50 datasets/...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster Analysis with DBscan\n",
    "#Step 1: to generate clusers we'll run script\n",
    "python cluster_vectors.py -c dbscan -e 0.5 -m 2 -i secrepo.h5 -o secrepo.h5\n",
    "\n",
    "#step 2: apply new hyperparameters with increased Eps setting from 5 to 6, producing less clusters \n",
    "python cluster_vectors.py -c dbscan -e -m 5 -i secrepo.h5 -o secrepo.h5\n",
    "\n",
    "#Stpe 3: Skip cluster insepction and validation and jump head to begin investigating bheavior of suspect samples. List samples \n",
    "python label_notes.py -i secrepo.h5 -l -1\n",
    "\n",
    "#Step 4: find out what IPs have bee doing using grep\n",
    "grep -ar 192.187.126.162 datasets/...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
